# TOP-50 queries by total time

# json_object - currently generated json
# prev_json_object - previously generated json

set -u -e -o pipefail

error_handler() {
  echo "^^^ ERROR at line: ['${BASH_LINENO[0]}']" >&2
  echo >&2
}
trap error_handler ERR

if [[ -z ${JSON_REPORTS_DIR+x} ]]; then
  echo "ERROR: this check can't be run as a single check or as a first check" >&2
  exit 1
fi

if [[ "${SSDBNAME}" != "None" ]]; then
  change_db_cmd="\connect ${SSDBNAME}
                "
else
  change_db_cmd=""
fi


tmp_dir="${JSON_REPORTS_DIR}/tmp_K003"
mkdir -p "${tmp_dir}"

results_cnt="0"
# fname_prefix generated by formula "json_files_cnt + 1"
for file in "${tmp_dir}"/[1-9]*_${ALIAS_INDEX}.json; do
  if [[ -f "${file}" ]]; then
    results_cnt=$(( results_cnt + 1 ))
  fi
done

fname_prefix=$(( results_cnt + 1 ))
prev_fname_prefix=$(( fname_prefix - 1 ))

# remove some symbols from observerd database name
simple_dbname=${DBNAME//[-, ,~,]/_}

cur_snapshot_fname="${tmp_dir}/${fname_prefix}_${simple_dbname}_${ALIAS_INDEX}.json"
prev_snapshot_fname="${tmp_dir}/${prev_fname_prefix}_${simple_dbname}_${ALIAS_INDEX}.json"

# read previous result and build prev_json_object
if [[ -f "${prev_snapshot_fname}" ]]; then
  prev_json_object=$(cat "${prev_snapshot_fname}")
fi

# check pg_stat_kcache availability
err_code="0"
res=$(${CHECK_HOST_CMD} "${_PSQL} -f -" <<'SQL' >/dev/null 2>&1
${change_db_cmd}
select from pg_stat_kcache limit 1 -- the fastest way
SQL
) || err_code="$?"

# main query to save statistics
if [[ "${err_code}" -ne "0" ]]; then
  # WITHOUT pg_stat_kcache
  QUERY="
    select
      /* rownum in snapshot may be not equal to resulting rownum */
      row_number() over (order by total_time desc) as rownum,
  
      /* pg_stat_statements_part */
      left(query, 50000) as query, /*  obsolete left ? check pg_stat_statements for cutting */
      calls,
      total_time,
      min_time,
      max_time,
      /*
      mean_time
      stddev_time
      */
      rows,
      shared_blks_hit,
      shared_blks_read,
      shared_blks_dirtied,
      shared_blks_written,
      local_blks_hit,
      local_blks_read,
      local_blks_dirtied,
      local_blks_written,
      temp_blks_read,
      temp_blks_written,
      blk_read_time,
      blk_write_time,
      /*
      save hash
      */
      md5( queryid::text || dbid::text || userid::text ) as md5
    from pg_stat_statements s
    order by total_time desc
    limit 50
  "
else
  # WITH pg_stat_kcache
  QUERY="
    select
      /* rownum in snapshot may be not equal to resulting rownum */
      row_number() over (order by total_time desc) as rownum,
  
      /* pg_stat_statements_part */
      left(query, 50000) as query, /*  obsolete left ? check pg_stat_statements for cutting */
      calls,
      total_time,
      min_time,
      max_time,
      /*
      mean_time
      stddev_time
      */
      rows,
      shared_blks_hit,
      shared_blks_read,
      shared_blks_dirtied,
      shared_blks_written,
      local_blks_hit,
      local_blks_read,
      local_blks_dirtied,
      local_blks_written,
      temp_blks_read,
      temp_blks_written,
      blk_read_time,
      blk_write_time,

      /* kcache part */
      k.reads as kcache_reads,
      k.writes as kcache_writes,
      k.user_time::bigint * 1000 as kcache_user_time_ms,
      k.system_time::bigint * 1000 as kcache_system_time_ms,
  
      /* save hash */
      md5(queryid::text || dbid::text || userid::text) as md5
    from pg_stat_statements s
    join pg_stat_kcache() k using(queryid, dbid, userid)
    order by total_time desc
    limit 50
  "
fi

# take snapshot and save as a json object
json_object=$(${CHECK_HOST_CMD} "${_PSQL} -f -" <<SQL
  ${change_db_cmd}
  with data as (
    ${QUERY}
  )
  select json_build_object(
    'snapshot_timestamptz'::text, to_json(now()::timestamptz)::json,
    'snapshot_timestamptz_s'::text, to_json(extract('epoch' from now()::timestamptz))::json,
    'queries', json_object_agg(data.md5, data.*)
  )
  from data
SQL
             )

# save to file
jq -r . <<<${json_object} > "${cur_snapshot_fname}"

res=""

if [[ "${prev_fname_prefix}" -eq "0" ]]; then
  echo "ERROR: need two checks to compare results. Please run whole check for this epoch again." >&2
  echo "NOTICE: ^^ this is not a real error. Just run check again." >&2
  exit 1
fi

# calculate time diff in seconds between checks
start_seconds=$(jq -r '.snapshot_timestamptz_s' "${prev_snapshot_fname}")
start_seconds_rnd=$(printf "%.0f\n" "${start_seconds}")
end_seconds=$(jq -r '.snapshot_timestamptz_s' "${cur_snapshot_fname}")
end_seconds_rnd=$(printf "%.0f\n" "${end_seconds}")

period_seconds_rnd=$(( end_seconds_rnd - start_seconds_rnd ))

if [[ "period_seconds_rnd" -le "0" ]]; then
  echo "ERROR: Period between snapshots is 0 seconds" >&2
  exit 1
fi

# generate sub_sql
sub_sql=" "
for key in \
           calls \
           total_time \
           rows \
           shared_blks_hit \
           shared_blks_read \
           shared_blks_dirtied \
           shared_blks_written \
           local_blks_hit \
           local_blks_read \
           local_blks_dirtied \
           local_blks_written \
           temp_blks_read \
           temp_blks_written \
           blk_read_time \
           blk_write_time \
           kcache_reads \
           kcache_writes \
           kcache_user_time_ms \
           kcache_system_time_ms \
           queryid \
           userid \
           dbid ;
                                   do
  sub_sql="${sub_sql}
    (s2.obj->>'${key}')::numeric - (s1.obj->>'${key}')::numeric as diff_${key},
    ( (s2.obj->>'${key}')::numeric - (s1.obj->>'${key}')::numeric ) / nullif(( select seconds from delta ), 0) as per_sec_${key},
    ( (s2.obj->>'${key}')::numeric - (s1.obj->>'${key}')::numeric ) / nullif(( (s2.obj->>'calls')::numeric - (s1.obj->>'calls')::numeric ), 0) as per_call_${key},
  "
done

sql="
  with snap1(j) as (
    select \$snap1\$
       ${prev_json_object}
    \$snap1\$::json
  ), snap2(j) as (
    select \$snap2\$
       ${json_object}
    \$snap2\$::json
  ), delta(seconds) as (
    select 
      (select j->>'snapshot_timestamptz_s' from snap2)::numeric
       - (select j->>'snapshot_timestamptz_s' from snap1)::numeric
  ), s1(md5, obj) as (
    select _.*
    from snap1, lateral json_each(j->'queries') as _
  ), s2(md5, obj) as (
    select _.*
    from snap2, lateral json_each(j->'queries') as _
  ), queries_pre as (
    select
      ${sub_sql}
      s1.md5 as md5,
      s1.obj->>'query' as query
    from s1
    join s2 using(md5)
  ), queries as (
    select
      row_number() over(order by diff_total_time desc) as rownum,
      *
    from queries_pre
    order by diff_total_time desc
  )
  select json_build_object(
    'start_timestamptz'::text, (select j->'snapshot_timestamptz' from snap1),
    'end_timestamptz'::text, (select j->'snapshot_timestamptz' from snap2),
    'period_seconds'::text, ( select (snap2.j->>'snapshot_timestamptz_s')::numeric - (snap1.j->>'snapshot_timestamptz_s')::numeric from snap1, snap2 ),
    'period_age'::text, ( select (snap2.j->>'snapshot_timestamptz')::timestamptz - (snap1.j->>'snapshot_timestamptz')::timestamptz from snap1, snap2 ),
    'queries', json_object_agg(queries.rownum, queries.*)
  )
  from queries
"
# sql result to stdout
${CHECK_HOST_CMD} "${_PSQL} -f -" <<SQL | jq -r .
  ${change_db_cmd}
  ${sql}
SQL


